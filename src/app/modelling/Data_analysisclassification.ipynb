{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5b4cdc-3614-47ba-9d5e-7024adb2ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7a8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, concat_ws, size, array, lit, when\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33aaac3-bb87-4c06-aa81-8f65457c03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22d9ac-16ec-471c-85c8-f66d2a7d1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 00:24:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/20 00:24:34 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/20 00:24:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "team = \"team3\"\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"{team} - Spark ML\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e47ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE team3_projectdb;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e582aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "moscow = spark.read.format(\"avro\").table(\"team3_projectdb.moscow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0014b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow = moscow.filter(\n",
    "    ~col(\"tags\").contains(\"'traffic_light'\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                 (0 + 1) / 1][Stage 52:>                 (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|   h3_09_center|       combined_tags|\n",
      "+---------------+--------------------+\n",
      "|89118180927ffff|[('crossing', 'tr...|\n",
      "|891181820abffff|[('highway', 'tra...|\n",
      "|891181844c3ffff|[('addr:country',...|\n",
      "|89118184c93ffff|[('sign', 'yes')]...|\n",
      "|89118186067ffff|[('barrier', 'gat...|\n",
      "|89118186173ffff|[('highway', 'tra...|\n",
      "|89118186233ffff|[('name', 'Давыдо...|\n",
      "|8911818635bffff|[('name', 'Клёнов...|\n",
      "|89118186493ffff|[('barrier', 'gat...|\n",
      "|8911818654fffff|[('name', 'Акулов...|\n",
      "+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "filtered_moscow = moscow.filter(col(\"h3_09_center\").isNotNull() & col(\"tags\").isNotNull())\n",
    "grouped_moscow = filtered_moscow.groupBy(\"h3_09_center\") \\\n",
    "    .agg(concat_ws(\" \", collect_list(col(\"tags\"))).alias(\"combined_tags\"))\n",
    "grouped_moscow.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd6a50c-c116-42f9-b05f-cda3470b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = spark.read.format(\"avro\").table(\"team3_projectdb.transactions\")\n",
    "cash_withdrawals = spark.read.format(\"avro\").table(\"team3_projectdb.cash_withdrawals\")\n",
    "locations = spark.read.format(\"avro\").table(\"team3_projectdb.locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "659bc11b-773c-4cdc-bfde-2ce8f041eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transactions.join(cash_withdrawals, [\"h3_09\", \"customer_id\"], \"inner\")\n",
    "data = data.join(locations, [\"h3_09\"], \"inner\").drop(\"lat\", \"lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71d140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+\n",
      "|          h3_09|customer_id|transaction_pk|count|    sum|     avg|  min|    max|      std|count_distinct|datetime_id|mcc_code|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4|3630.75|907.6875|423.0|1825.92| 640.2593|             2|          3|      13|\n",
      "|8911aa7abd3ffff|        196|           119|   11|4172.97|379.3609| 93.0|  927.0|266.54895|             4|          3|      13|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24002431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- transaction_pk: long (nullable = true)\n",
      " |-- count: short (nullable = true)\n",
      " |-- sum: float (nullable = true)\n",
      " |-- avg: float (nullable = true)\n",
      " |-- min: float (nullable = true)\n",
      " |-- max: float (nullable = true)\n",
      " |-- std: float (nullable = true)\n",
      " |-- count_distinct: short (nullable = true)\n",
      " |-- datetime_id: short (nullable = true)\n",
      " |-- mcc_code: short (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a928d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- combined_tags: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_moscow = grouped_moscow.withColumnRenamed(\"h3_09_center\", \"h3_09\")\n",
    "grouped_moscow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "552b20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(\n",
    "    grouped_moscow,\n",
    "    [\"h3_09\"],  \n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08655ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = data.withColumn(\n",
    "    \"tokens\", \n",
    "    split(col(\"combined_tags\"), \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d2c617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- transaction_pk: long (nullable = true)\n",
      " |-- count: short (nullable = true)\n",
      " |-- sum: float (nullable = true)\n",
      " |-- avg: float (nullable = true)\n",
      " |-- min: float (nullable = true)\n",
      " |-- max: float (nullable = true)\n",
      " |-- std: float (nullable = true)\n",
      " |-- count_distinct: short (nullable = true)\n",
      " |-- datetime_id: short (nullable = true)\n",
      " |-- mcc_code: short (nullable = true)\n",
      " |-- combined_tags: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae17570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+\n",
      "|          h3_09|customer_id|transaction_pk|count|    sum|     avg|  min|    max|      std|count_distinct|datetime_id|mcc_code|       combined_tags|              tokens|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4|3630.75|907.6875|423.0|1825.92| 640.2593|             2|          3|      13|[('amenity', 'pha...|[[('amenity',, 'p...|\n",
      "|8911aa7abd3ffff|        196|           119|   11|4172.97|379.3609| 93.0|  927.0|266.54895|             4|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|\n",
      "|8911aa7a363ffff|        269|           164|    5|  343.0|    68.6| 37.0|  148.0|47.125366|             1|          3|      13|[('colour', 'red'...|[[('colour',, 're...|\n",
      "|8911aa7ad67ffff|        368|           225|    4|1510.01|377.5025| 35.9|  969.0|407.77036|             4|          3|      13|[('barrier', 'ker...|[[('barrier',, 'k...|\n",
      "|8911aa6ae5bffff|        545|           348|    1|  91.78|   91.78|91.78|  91.78|     NULL|             1|          3|      13|[('check_date:tac...|[[('check_date:ta...|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "prepared_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16f81f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Tokenizer, HashingTF\n",
    "tokenizer = Tokenizer(inputCol=\"token\", outputCol=\"tokenized\")\n",
    "hashingTF = HashingTF(inputCol=\"tokenized\", outputCol=\"tags_vectors\", numFeatures=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b9a330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepared_data.withColumn(\n",
    "    \"token\", \n",
    "    concat_ws(\" \", col(\"tokens\"))  # Объединяем элементы массива через пробел\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17cee225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+--------+---------+------+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n",
      "|          h3_09|customer_id|transaction_pk|count|     sum|      avg|   min|    max|      std|count_distinct|datetime_id|mcc_code|       combined_tags|              tokens|               token|\n",
      "+---------------+-----------+--------------+-----+--------+---------+------+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4| 3630.75| 907.6875| 423.0|1825.92| 640.2593|             2|          3|      13|[('amenity', 'pha...|[[('amenity',, 'p...|[('amenity', 'pha...|\n",
      "|8911aa7abd3ffff|        196|           119|   11| 4172.97| 379.3609|  93.0|  927.0|266.54895|             4|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|[('alt_name:en', ...|\n",
      "|8911aa7a363ffff|        269|           164|    5|   343.0|     68.6|  37.0|  148.0|47.125366|             1|          3|      13|[('colour', 'red'...|[[('colour',, 're...|[('colour', 'red'...|\n",
      "|8911aa7ad67ffff|        368|           225|    4| 1510.01| 377.5025|  35.9|  969.0|407.77036|             4|          3|      13|[('barrier', 'ker...|[[('barrier',, 'k...|[('barrier', 'ker...|\n",
      "|8911aa6ae5bffff|        545|           348|    1|   91.78|    91.78| 91.78|  91.78|     NULL|             1|          3|      13|[('check_date:tac...|[[('check_date:ta...|[('check_date:tac...|\n",
      "|8911aa7abd3ffff|        630|           408|    9| 5650.97|627.88556| 203.0|1368.04|417.28323|             7|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|[('alt_name:en', ...|\n",
      "|8911aa7a963ffff|        703|           455|    1|  176.88|   176.88|176.88| 176.88|     NULL|             1|          3|      13|[('highway', 'tra...|[[('highway',, 't...|[('highway', 'tra...|\n",
      "|8911aa634b3ffff|        741|           481|   18|34826.43|1934.8016| 799.9|4890.92|1112.6428|            10|          3|      13|[('amenity', 'ban...|[[('amenity',, 'b...|[('amenity', 'ban...|\n",
      "|8911aa4d96fffff|        783|           507|    1|   273.0|    273.0| 273.0|  273.0|     NULL|             1|          3|      13|[('colour', '#a0a...|[[('colour',, '#a...|[('colour', '#a0a...|\n",
      "|8911aa6b473ffff|        815|           527|    5|  2178.7|   435.74|  58.0|1836.76| 783.7069|             4|          3|      13|[('amenity', 'res...|[[('amenity',, 'r...|[('amenity', 'res...|\n",
      "+---------------+-----------+--------------+-----+--------+---------+------+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "prepared_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ac6d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenizer.transform(prepared_data)\n",
    "result_df = hashingTF.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e61e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          h3_09|customer_id|transaction_pk|count|    sum|     avg|  min|    max|      std|count_distinct|datetime_id|mcc_code|       combined_tags|              tokens|               token|           tokenized|        tags_vectors|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4|3630.75|907.6875|423.0|1825.92| 640.2593|             2|          3|      13|[('amenity', 'pha...|[[('amenity',, 'p...|[('amenity', 'pha...|[[('amenity',, 'p...|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa7abd3ffff|        196|           119|   11|4172.97|379.3609| 93.0|  927.0|266.54895|             4|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|[('alt_name:en', ...|[[('alt_name:en',...|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa7a363ffff|        269|           164|    5|  343.0|    68.6| 37.0|  148.0|47.125366|             1|          3|      13|[('colour', 'red'...|[[('colour',, 're...|[('colour', 'red'...|[[('colour',, 're...|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa7ad67ffff|        368|           225|    4|1510.01|377.5025| 35.9|  969.0|407.77036|             4|          3|      13|[('barrier', 'ker...|[[('barrier',, 'k...|[('barrier', 'ker...|[[('barrier',, 'k...|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa6ae5bffff|        545|           348|    1|  91.78|   91.78|91.78|  91.78|     NULL|             1|          3|      13|[('check_date:tac...|[[('check_date:ta...|[('check_date:tac...|[[('check_date:ta...|(25,[0,1,2,3,4,5,...|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.drop(\"combined_tags\", \"tokens\", \"token\", \"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4db4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                 (0 + 1) / 1][Stage 77:>                 (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# word2vec = Word2Vec(\n",
    "#     vectorSize=50,\n",
    "#     minCount=1,\n",
    "#     inputCol=\"tokens\",\n",
    "#     outputCol=\"embedding\"\n",
    "# )\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=25,         \n",
    "    minCount=3,             \n",
    "    windowSize=3,           \n",
    "    inputCol=\"tokens\",\n",
    "    outputCol=\"embedding\",\n",
    "    stepSize=0.01          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e16d15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empty_tokens_count = prepared_data.filter(\n",
    "    (size(col(\"tokens\")) == 0) |  \n",
    "    col(\"tokens\").isNull()       \n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6127fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(empty_tokens_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0e1e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_VALUE = [\"unknown\"]\n",
    "prepared_data = prepared_data.withColumn(\n",
    "    \"tokens\",\n",
    "    when(\n",
    "        (size(col(\"tokens\")) == 0) | col(\"tokens\").isNull(),  # Исправлены скобки\n",
    "        array([lit(e) for e in FIXED_VALUE])                   # Правильное создание массива\n",
    "    ).otherwise(col(\"tokens\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2cc978e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: FeatureHasher requires columns to be of numeric, boolean or string. Column tokens was array<string>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m hasher \u001b[38;5;241m=\u001b[39m FeatureHasher(\n\u001b[1;32m      5\u001b[0m     inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Укажите столбцы для хеширования\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     numFeatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m  \u001b[38;5;66;03m# Желаемая размерность эмбеддингов\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Прямое преобразование данных БЕЗ .fit()\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mhasher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Проверка результата\u001b[39;00m\n\u001b[1;32m     14\u001b[0m result_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/BigData-project/.venv/lib/python3.11/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/BigData-project/.venv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/BigData-project/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/BigData-project/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: FeatureHasher requires columns to be of numeric, boolean or string. Column tokens was array<string>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:>                 (0 + 1) / 1][Stage 87:>                 (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "# Создание экземпляра FeatureHasher\n",
    "hasher = FeatureHasher(\n",
    "    inputCols=[\"tokens\"],  # Укажите столбцы для хеширования\n",
    "    outputCol=\"embedding\",\n",
    "    numFeatures=25  # Желаемая размерность эмбеддингов\n",
    ")\n",
    "\n",
    "# Прямое преобразование данных БЕЗ .fit()\n",
    "result_df = hasher.transform(prepared_data)\n",
    "\n",
    "# Проверка результата\n",
    "result_df.select(\"embedding\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = model.transform(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37a0d18-49d4-4512-84b1-03fff4476172",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features = [\n",
    "    \"datetime_id\", \"count\", \"sum\", \n",
    "    \"avg\", \"min\", \"max\", \"std\",\n",
    "    \"count_distinct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b16cf13-e795-4035-99ad-bd864716acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.na.drop(subset=original_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5a0dbe-360b-42f0-aa02-c93b2e4076c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [\"h3_09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8a5aca-6dd5-4844-89f4-6e6ed499350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "label_indexer = StringIndexer(inputCol=\"h3_09\", outputCol=\"label\").fit(data)\n",
    "data = label_indexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e9cf062-22aa-4c67-a232-5024f31a7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"label\", outputCol=f\"{column}_encoded\")\n",
    "    for column in string_columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3747cfe-6c5f-4f9e-a410-8232bc43adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_pipeline = Pipeline(stages=encoders)\n",
    "encoded_data = encoding_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121b0e3b-10cb-42a4-87c9-272178c96d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(h3_09='8911aa7a6d3ffff', customer_id=107, transaction_pk=61, count=4, sum=3630.75, avg=907.6875, min=423.0, max=1825.9200439453125, std=640.25927734375, count_distinct=2, datetime_id=3, mcc_code=13, label=122.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ac5c3b-9961-4a09-bb55-3c6b1b6e3e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: bigint, transaction_pk: bigint, count: smallint, sum: float, avg: float, min: float, max: float, std: float, count_distinct: smallint, datetime_id: smallint, mcc_code: smallint, label: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(\"h3_09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f3bae7-026d-4ab1-8508-40afc3fec215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_indexer = StringIndexer(inputCol=\"mcc_code\", outputCol=\"mcc_code_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422500f0-f199-4f6d-8e50-a3b1d9439949",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = original_features + [\"mcc_code_index\"]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2abe8d3e-8014-4b73-b569-ceb1cb60bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8f8fa1e-827f-422e-9da9-2aa3a8805bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=25,       \n",
    "    maxDepth=7,        \n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "762335d3-f107-46ae-b11b-80c8329d3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=[mcc_indexer, assembler, lr])\n",
    "pipeline_rf = Pipeline(stages=[mcc_indexer, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2ef9ea-dce4-4002-91bc-8ff5da209d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03e0b5c4-4a1a-4767-be6c-7861c4808d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(h3_09='8911818610bffff', customer_id=1924, transaction_pk=1143088, count=2, sum=6395.0, avg=3197.5, min=1198.0, max=5197.0, std=2827.719970703125, count_distinct=2, datetime_id=2, mcc_code=10, label=832.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aef82f69-73e1-42ab-b0b0-1688a59bf6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = pipeline_rf.fit(train_data)\n",
    "predictions_rf = model_rf.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "571e1ee5-39e7-4c51-816e-bb9a91b107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbb4db29-9a4f-46e8-b978-18d440fafc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Results:\n",
      "Accuracy: 0.1786409567071475\n",
      "F1-Score: 0.0689351678368335\n"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest Results:\")\n",
    "print(f\"Accuracy: {evaluator_accuracy.evaluate(predictions_rf)}\")\n",
    "print(f\"F1-Score: {evaluator_f1.evaluate(predictions_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbbf65ff-9d0a-4ee3-9a4b-172a1af9cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = pipeline_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a35fd552-2e61-4fe8-a54b-08e17e308743",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03d5916d-a5cc-4e68-a6ce-3d72c099aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16850529958901148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.055522637971441635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {evaluator_accuracy.evaluate(lr_predictions)}\")\n",
    "print(f\"F1-Score: {evaluator_f1.evaluate(lr_predictions)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263793a-7eeb-41a7-884c-a3c8eb759779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_converter = IndexToString(\n",
    "#     inputCol=\"prediction\",\n",
    "#     outputCol=\"predicted_h3_09\",\n",
    "#     labels=label_indexer.labels\n",
    "# )\n",
    "\n",
    "# final_predictions = label_converter.transform(gbt_predictions)\n",
    "# final_predictions.select(\"h3_09\", \"predicted_h3_09\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d29e549-f15e-4c36-b7dd-67ff07522d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.7 (main, Mar 20 2025, 00:23:21) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bcf360-a3cf-4000-a2be-50bc4cf12420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/team3/.local/lib/python3.6/site-packages (1.10.1)\n",
      "Requirement already satisfied: torchvision in /home/team3/.local/lib/python3.6/site-packages (0.11.2)\n",
      "Requirement already satisfied: pytorch_lightning in /home/team3/.local/lib/python3.6/site-packages (1.5.10.post0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/team3/.local/lib/python3.6/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: setuptools==59.5.0 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib64/python3.6/site-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.6/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/site-packages (from pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (2.10.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/site-packages (from pytorch_lightning) (4.64.1)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (2022.1.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (0.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /home/team3/.local/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging>=17.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.7)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.48.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.3)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/site-packages (from tqdm>=4.41.0->pytorch_lightning) (5.4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/team3/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/team3/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/team3/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.9.1)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (1.26.18)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/team3/.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/team3/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.41.0->pytorch_lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/team3/.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user torch torchvision pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c180b62-49ae-41cd-88be-1c3375f31089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu102\n",
      "0.11.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)         # Должно быть 1.13.1\n",
    "print(torchvision.__version__)   # Должно быть 0.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "470241ab-6ecd-4373-a4f6-bc1bf088160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5f13ef-1e73-4ecc-a92a-7755d32872cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # Должен показывать путь к Python 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9f56947-3bd6-483d-b8ca-a6414444e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/team3/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/team3/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/team3/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_17032/610935235.py\", line 3, in <module>\n",
      "    from torchvision import models\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "/home/team3/BigData-project/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309cd77e-17eb-482c-881f-219a82bcfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = np.array(data.select(\"features\").collect()).squeeze()\n",
    "        self.labels = np.array(data.select(\"label\").collect()).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]]).squeeze()\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d214036-0980-4312-abcc-5e11c6958993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.01)\n",
    "        self.fc2 = nn.Linear(2100, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a285ed3-f9d9-4986-bd76-1a778c341c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline(stages=[mcc_indexer, assembler])\n",
    "preprocessed_data = preprocessing_pipeline.fit(data).transform(data)\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = np.array(data.select(\"features\").collect()).squeeze()\n",
    "        self.labels = np.array(data.select(\"label\").collect()).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]]).squeeze()\n",
    "        return features, label\n",
    "\n",
    "train_dataset = SparkDataset(train_data)\n",
    "test_dataset = SparkDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cce770a5-2047-4143-a4b0-845a30af8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(feature_cols)\n",
    "num_classes = data.select(\"label\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a171cf3-14ff-4bf0-ac12-354647e3a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNN(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8b1a53d-c9c1-4334-beef-415bc3a43a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b7fe0d6-4eb2-4bad-8f59-727625a0acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d878c1-077d-473b-be9b-8966643707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22953be6-34f6-44c8-a4ba-f34c2bd1d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Accuracy: 16.45%\n",
      "Epoch [2/10], Accuracy: 16.78%\n",
      "Epoch [3/10], Accuracy: 16.80%\n",
      "Epoch [4/10], Accuracy: 16.77%\n",
      "Epoch [5/10], Accuracy: 16.82%\n",
      "Epoch [6/10], Accuracy: 16.81%\n",
      "Epoch [7/10], Accuracy: 16.87%\n",
      "Epoch [8/10], Accuracy: 16.81%\n",
      "Epoch [9/10], Accuracy: 16.67%\n",
      "Epoch [10/10], Accuracy: 16.85%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f2fbde7-59c6-4a56-8ed8-12764d2e3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col, struct \n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def predict_batch(batch_iter):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for features, _ in batch_iter:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.numpy())\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "predict_udf = pandas_udf(predict_batch, returnType=DoubleType())\n",
    "\n",
    "final_predictions = test_data.withColumn(\n",
    "    \"nn_prediction\",\n",
    "    predict_udf(struct(*feature_cols))  \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "829c3659-32cc-4a08-8641-5f756b485e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "final_predictions = final_predictions.withColumnRenamed(\"nn_prediction\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acab72c6-2071-49e9-9771-8581f121a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4b0e941-5ac3-486e-9e5d-6883fe3f0d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyarrow\n",
    "print(pyarrow.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fc867-f3e7-4f49-941d-af9cf26d28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator_accuracy.evaluate(final_predictions)\n",
    "f1 = evaluator_f1.evaluate(final_predictions)\n",
    "\n",
    "print(\"Neural Network Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a59754f5-dbb4-483c-916f-3e0d2c6983a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8007639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "851f0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TabularResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.block1 = self._make_residual_block(input_size, 512)\n",
    "        self.block2 = self._make_residual_block(512, 256)\n",
    "        self.block3 = self._make_residual_block(256, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def _make_residual_block(self, in_features, out_features):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.block3(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8a2c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_size = len(feature_cols) \n",
    "num_classes = data.select(\"label\").distinct().count()\n",
    "model = TabularResNet(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1030d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# def prepare_data(df):\n",
    "#     features = np.stack(df.select(\"features\").toPandas()['features'].apply(lambda x: x.toArray()))\n",
    "#     labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "#     return SparkDataset(features, labels)\n",
    "\n",
    "# train_dataset = prepare_data(train_data)\n",
    "# test_dataset = prepare_data(test_data)\n",
    "\n",
    "preprocessing_pipeline = Pipeline(stages=[mcc_indexer, assembler])\n",
    "preprocessed_data = preprocessing_pipeline.fit(data).transform(data)\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = np.array(data.select(\"features\").collect()).squeeze()\n",
    "        self.labels = np.array(data.select(\"label\").collect()).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]]).squeeze()\n",
    "        return features, label\n",
    "\n",
    "train_dataset = SparkDataset(train_data)\n",
    "test_dataset = SparkDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1ab1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6388294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4.8850\n",
      "Epoch 2/20, Loss: 4.8808\n",
      "Epoch 3/20, Loss: 4.8779\n",
      "Epoch 4/20, Loss: 4.8727\n",
      "Epoch 5/20, Loss: 4.8666\n",
      "Epoch 6/20, Loss: 4.8685\n",
      "Epoch 7/20, Loss: 4.8588\n",
      "Epoch 8/20, Loss: 4.8605\n",
      "Epoch 9/20, Loss: 4.8528\n",
      "Epoch 10/20, Loss: 4.8537\n",
      "Epoch 11/20, Loss: 4.8497\n",
      "Epoch 12/20, Loss: 4.8400\n",
      "Epoch 13/20, Loss: 4.8422\n",
      "Epoch 14/20, Loss: 4.8351\n",
      "Epoch 15/20, Loss: 4.8367\n",
      "Epoch 16/20, Loss: 4.8262\n",
      "Epoch 17/20, Loss: 4.8266\n",
      "Epoch 18/20, Loss: 4.8223\n",
      "Epoch 19/20, Loss: 4.8158\n",
      "Epoch 20/20, Loss: 4.8140\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7762abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20ed7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c58a803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Defaulting to user installation because normal site-packages is not writeable',\n",
       " 'Collecting numpy',\n",
       " '  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)',\n",
       " 'Installing collected packages: numpy',\n",
       " 'Successfully installed numpy-1.19.5']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0594e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1656, F1-Score: 0.0002\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            preds = output.argmax(dim=1)\n",
    "            \n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(target.cpu().tolist())\n",
    "    \n",
    "    \n",
    "    correct = sum(1 for p, l in zip(all_preds, all_labels) if p == l)\n",
    "    accuracy = correct / len(all_labels)\n",
    "    \n",
    "    \n",
    "    unique_labels = set(all_labels)\n",
    "    f1_scores = []\n",
    "    for label in unique_labels:\n",
    "        TP = sum((p == label) and (l == label) for p, l in zip(all_preds, all_labels))\n",
    "        FP = sum((p == label) and (l != label) for p, l in zip(all_preds, all_labels))\n",
    "        FN = sum((p != label) and (l == label) for p, l in zip(all_preds, all_labels))\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    \n",
    "    return accuracy, avg_f1\n",
    "\n",
    "\n",
    "accuracy, f1 = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc2f178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
