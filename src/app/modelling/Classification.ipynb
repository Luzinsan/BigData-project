{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c5b4cdc-3614-47ba-9d5e-7024adb2ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b7a8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, concat_ws, size, array, lit, when\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "974b19b7-2fdd-4131-b013-29f39420a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 1.8.0_402\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2025-02-23T20:30:46Z\n",
      "Revision 7c29c664cdc9321205a98a14858aaf8daaa19db2\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a22d9ac-16ec-471c-85c8-f66d2a7d1f86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/20 13:18:28 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/20 13:18:28 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "team = \"team3\"\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"{team} - Spark ML\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09a54629-c1d4-4006-9f0b-42cd43a0e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e47ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE team3_projectdb;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e582aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow = spark.read.format(\"parquet\").table(\"team3_projectdb.q6_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1470327d-c154-4660-a7a0-70921e8ede9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- h3_09_center: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- place_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moscow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a966c9c-98d7-4473-8fe3-883493ad8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 14:50:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------------------------------------------------------------------+\n",
      "|h3_09_center   |place_names                                                                              |\n",
      "+---------------+-----------------------------------------------------------------------------------------+\n",
      "|8911aa0f02fffff|[пункт выдачи инвентаря, пункт выдачи инвентаря]                                         |\n",
      "|891181b055bffff|[кск \"аврора\"]                                                                           |\n",
      "|8911aa70c6fffff|[гу ис]                                                                                  |\n",
      "|8911aa48d17ffff|[продукты мини маркет]                                                                   |\n",
      "|8911aa46d83ffff|[car wash, car wash]                                                                     |\n",
      "|891181b0d13ffff|[мангал, дом 6, фруктовая, десантное поле, цзс, карьер]                                  |\n",
      "|8911aa42677ffff|[гск \"лада-спутник\", дом священника]                                                     |\n",
      "|8911aa00ec3ffff|[зеленоградстрой, мосэнергосбыт]                                                         |\n",
      "|8911aa48a57ffff|[поле ромашек]                                                                           |\n",
      "|8911aa61257ffff|[киберарена]                                                                             |\n",
      "|891181b290fffff|[бти]                                                                                    |\n",
      "|8911aa6658bffff|[бухта тихая, зао «домостроитель»]                                                       |\n",
      "|891181b2a3bffff|[снт позсо 49/1, снт сокол, снт часовщик 55, снт москвич, снт 50/2 позсо, снт вторчермет]|\n",
      "|891181b293bffff|[военно-учётный стол]                                                                    |\n",
      "|8911aa01183ffff|[кпп]                                                                                    |\n",
      "|8911aa7ab07ffff|[мострансагенство]                                                                       |\n",
      "|8911aa49bdbffff|[садовый квартал]                                                                        |\n",
      "|8911aa61d07ffff|[королевские лоси]                                                                       |\n",
      "|891181954b3ffff|[львовский таможенный пост]                                                              |\n",
      "|891181b6d97ffff|[marine bouy 55]                                                                         |\n",
      "+---------------+-----------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"{team}\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"USE team3_projectdb;\")\n",
    "moscow = spark.read.format(\"parquet\").table(\"team3_projectdb.q6_results\")\n",
    "\n",
    "top_centers = moscow.groupBy(\"h3_09_center\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .limit(30)\n",
    "\n",
    "moscow.join(top_centers, \"h3_09_center\") \\\n",
    "    .groupBy(\"h3_09_center\") \\\n",
    "    .agg(collect_list(\"place_name\").alias(\"place_names\")) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0014b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moscow = moscow.filter(\n",
    "#     ~col(\"tags\").contains(\"'traffic_light'\") \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894cb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_moscow = moscow.filter(col(\"h3_09_center\").isNotNull() & col(\"tags\").isNotNull())\n",
    "# grouped_moscow = filtered_moscow.groupBy(\"h3_09_center\") \\\n",
    "#     .agg(concat_ws(\" \", collect_list(col(\"tags\"))).alias(\"combined_tags\"))\n",
    "# moscow.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd6a50c-c116-42f9-b05f-cda3470b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = spark.read.format(\"parquet\").table(\"team3_projectdb.transactions\")\n",
    "cash_withdrawals = spark.read.format(\"parquet\").table(\"team3_projectdb.cash_withdrawals\")\n",
    "locations = spark.read.format(\"parquet\").table(\"team3_projectdb.locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659bc11b-773c-4cdc-bfde-2ce8f041eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transactions.join(cash_withdrawals, [\"h3_09\", \"customer_id\"], \"inner\")\n",
    "data = data.join(locations, [\"h3_09\"], \"inner\").drop(\"lat\", \"lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d71d140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+\n",
      "|          h3_09|customer_id|transaction_pk|count|    sum|     avg|  min|    max|      std|count_distinct|datetime_id|mcc_code|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4|3630.75|907.6875|423.0|1825.92| 640.2593|             2|          3|      13|\n",
      "|8911aa7abd3ffff|        196|           119|   11|4172.97|379.3609| 93.0|  927.0|266.54895|             4|          3|      13|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24002431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- transaction_pk: long (nullable = true)\n",
      " |-- count: short (nullable = true)\n",
      " |-- sum: float (nullable = true)\n",
      " |-- avg: float (nullable = true)\n",
      " |-- min: float (nullable = true)\n",
      " |-- max: float (nullable = true)\n",
      " |-- std: float (nullable = true)\n",
      " |-- count_distinct: short (nullable = true)\n",
      " |-- datetime_id: short (nullable = true)\n",
      " |-- mcc_code: short (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a928d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- combined_tags: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_moscow = grouped_moscow.withColumnRenamed(\"h3_09_center\", \"h3_09\")\n",
    "grouped_moscow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552b20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(\n",
    "    grouped_moscow,\n",
    "    [\"h3_09\"],  \n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08655ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = data.withColumn(\n",
    "    \"tokens\", \n",
    "    split(col(\"combined_tags\"), \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d2c617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- h3_09: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- transaction_pk: long (nullable = true)\n",
      " |-- count: short (nullable = true)\n",
      " |-- sum: float (nullable = true)\n",
      " |-- avg: float (nullable = true)\n",
      " |-- min: float (nullable = true)\n",
      " |-- max: float (nullable = true)\n",
      " |-- std: float (nullable = true)\n",
      " |-- count_distinct: short (nullable = true)\n",
      " |-- datetime_id: short (nullable = true)\n",
      " |-- mcc_code: short (nullable = true)\n",
      " |-- combined_tags: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae17570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+\n",
      "|          h3_09|customer_id|transaction_pk|count|    sum|     avg|  min|    max|      std|count_distinct|datetime_id|mcc_code|       combined_tags|              tokens|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4|3630.75|907.6875|423.0|1825.92| 640.2593|             2|          3|      13|[('amenity', 'pha...|[[('amenity',, 'p...|\n",
      "|8911aa7abd3ffff|        196|           119|   11|4172.97|379.3609| 93.0|  927.0|266.54895|             4|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|\n",
      "|8911aa7a363ffff|        269|           164|    5|  343.0|    68.6| 37.0|  148.0|47.125366|             1|          3|      13|[('colour', 'red'...|[[('colour',, 're...|\n",
      "|8911aa7ad67ffff|        368|           225|    4|1510.01|377.5025| 35.9|  969.0|407.77036|             4|          3|      13|[('barrier', 'ker...|[[('barrier',, 'k...|\n",
      "|8911aa6ae5bffff|        545|           348|    1|  91.78|   91.78|91.78|  91.78|     NULL|             1|          3|      13|[('check_date:tac...|[[('check_date:ta...|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "prepared_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f81f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Tokenizer, HashingTF\n",
    "tokenizer = Tokenizer(inputCol=\"token\", outputCol=\"tokenized\")\n",
    "hashingTF = HashingTF(inputCol=\"tokenized\", outputCol=\"tags_vectors\", numFeatures=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9a330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepared_data.withColumn(\n",
    "    \"token\", \n",
    "    concat_ws(\" \", col(\"tokens\"))  # Объединяем элементы массива через пробел\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17cee225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+--------+---------+------+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n",
      "|          h3_09|customer_id|transaction_pk|count|     sum|      avg|   min|    max|      std|count_distinct|datetime_id|mcc_code|       combined_tags|              tokens|               token|\n",
      "+---------------+-----------+--------------+-----+--------+---------+------+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4| 3630.75| 907.6875| 423.0|1825.92| 640.2593|             2|          3|      13|[('amenity', 'pha...|[[('amenity',, 'p...|[('amenity', 'pha...|\n",
      "|8911aa7abd3ffff|        196|           119|   11| 4172.97| 379.3609|  93.0|  927.0|266.54895|             4|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|[('alt_name:en', ...|\n",
      "|8911aa7a363ffff|        269|           164|    5|   343.0|     68.6|  37.0|  148.0|47.125366|             1|          3|      13|[('colour', 'red'...|[[('colour',, 're...|[('colour', 'red'...|\n",
      "|8911aa7ad67ffff|        368|           225|    4| 1510.01| 377.5025|  35.9|  969.0|407.77036|             4|          3|      13|[('barrier', 'ker...|[[('barrier',, 'k...|[('barrier', 'ker...|\n",
      "|8911aa6ae5bffff|        545|           348|    1|   91.78|    91.78| 91.78|  91.78|     NULL|             1|          3|      13|[('check_date:tac...|[[('check_date:ta...|[('check_date:tac...|\n",
      "|8911aa7abd3ffff|        630|           408|    9| 5650.97|627.88556| 203.0|1368.04|417.28323|             7|          3|      13|[('alt_name:en', ...|[[('alt_name:en',...|[('alt_name:en', ...|\n",
      "|8911aa7a963ffff|        703|           455|    1|  176.88|   176.88|176.88| 176.88|     NULL|             1|          3|      13|[('highway', 'tra...|[[('highway',, 't...|[('highway', 'tra...|\n",
      "|8911aa634b3ffff|        741|           481|   18|34826.43|1934.8016| 799.9|4890.92|1112.6428|            10|          3|      13|[('amenity', 'ban...|[[('amenity',, 'b...|[('amenity', 'ban...|\n",
      "|8911aa4d96fffff|        783|           507|    1|   273.0|    273.0| 273.0|  273.0|     NULL|             1|          3|      13|[('colour', '#a0a...|[[('colour',, '#a...|[('colour', '#a0a...|\n",
      "|8911aa6b473ffff|        815|           527|    5|  2178.7|   435.74|  58.0|1836.76| 783.7069|             4|          3|      13|[('amenity', 'res...|[[('amenity',, 'r...|[('amenity', 'res...|\n",
      "+---------------+-----------+--------------+-----+--------+---------+------+-------+---------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac6d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenizer.transform(prepared_data)\n",
    "result_df = hashingTF.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e61e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+\n",
      "|          h3_09|customer_id|transaction_pk|count|    sum|     avg|  min|    max|      std|count_distinct|datetime_id|mcc_code|        tags_vectors|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+\n",
      "|8911aa7a6d3ffff|        107|            61|    4|3630.75|907.6875|423.0|1825.92| 640.2593|             2|          3|      13|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa7abd3ffff|        196|           119|   11|4172.97|379.3609| 93.0|  927.0|266.54895|             4|          3|      13|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa7a363ffff|        269|           164|    5|  343.0|    68.6| 37.0|  148.0|47.125366|             1|          3|      13|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa7ad67ffff|        368|           225|    4|1510.01|377.5025| 35.9|  969.0|407.77036|             4|          3|      13|(25,[0,1,2,3,4,5,...|\n",
      "|8911aa6ae5bffff|        545|           348|    1|  91.78|   91.78|91.78|  91.78|     NULL|             1|          3|      13|(25,[0,1,2,3,4,5,...|\n",
      "+---------------+-----------+--------------+-----+-------+--------+-----+-------+---------+--------------+-----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3676a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.drop(\"combined_tags\", \"tokens\", \"token\", \"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e16d15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empty_tokens_count = prepared_data.filter(\n",
    "    (size(col(\"tokens\")) == 0) |  \n",
    "    col(\"tokens\").isNull()       \n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6127fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(empty_tokens_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = model.transform(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b37a0d18-49d4-4512-84b1-03fff4476172",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features = [\n",
    "    \"datetime_id\", \"count\", \"sum\", \n",
    "    \"avg\", \"min\", \"max\", \"std\",\n",
    "    \"count_distinct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f5a0dbe-360b-42f0-aa02-c93b2e4076c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [\"h3_09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec8a5aca-6dd5-4844-89f4-6e6ed499350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# label_indexer = StringIndexer(inputCol=\"h3_09\", outputCol=\"label\").fit(data)\n",
    "# data = label_indexer.transform(data)\n",
    "label_indexer = StringIndexer(inputCol=\"h3_09\", outputCol=\"label\").fit(result_df)\n",
    "result_df = label_indexer.transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c5efea7-a292-4e29-960e-5dbf61138b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(h3_09='8911aa7a6d3ffff', customer_id=107, transaction_pk=61, count=4, sum=3630.75, avg=907.6875, min=423.0, max=1825.9200439453125, std=640.25927734375, count_distinct=2, datetime_id=3, mcc_code=13, tags_vectors=SparseVector(25, {0: 11.0, 1: 1.0, 2: 15.0, 3: 32.0, 4: 32.0, 5: 16.0, 6: 16.0, 7: 4.0, 8: 32.0, 9: 3.0, 10: 1.0, 11: 5.0, 12: 11.0, 13: 16.0, 14: 9.0, 15: 9.0, 16: 15.0, 17: 14.0, 18: 6.0, 19: 6.0, 20: 11.0, 21: 2.0, 22: 23.0, 23: 9.0, 24: 44.0}), label=141.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121b0e3b-10cb-42a4-87c9-272178c96d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(h3_09='8911aa7a6d3ffff', customer_id=107, transaction_pk=61, count=4, sum=3630.75, avg=907.6875, min=423.0, max=1825.9200439453125, std=640.25927734375, count_distinct=2, datetime_id=3, mcc_code=13, label=122.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7ac5c3b-9961-4a09-bb55-3c6b1b6e3e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: bigint, transaction_pk: bigint, count: smallint, sum: float, avg: float, min: float, max: float, std: float, count_distinct: smallint, datetime_id: smallint, mcc_code: smallint, tags_vectors: vector, label: double]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.drop(\"h3_09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9f3bae7-026d-4ab1-8508-40afc3fec215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_indexer = StringIndexer(inputCol=\"mcc_code\", outputCol=\"mcc_code_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "422500f0-f199-4f6d-8e50-a3b1d9439949",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = original_features + [\"mcc_code_index\"]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2abe8d3e-8014-4b73-b569-ceb1cb60bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8f8fa1e-827f-422e-9da9-2aa3a8805bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=25,       \n",
    "    maxDepth=7,        \n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "762335d3-f107-46ae-b11b-80c8329d3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=[mcc_indexer, assembler, lr])\n",
    "pipeline_rf = Pipeline(stages=[mcc_indexer, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a2ef9ea-dce4-4002-91bc-8ff5da209d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = result_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03e0b5c4-4a1a-4767-be6c-7861c4808d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef82f69-73e1-42ab-b0b0-1688a59bf6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "25/05/20 03:02:14 WARN DAGScheduler: Broadcasting large task binary with size 1159.3 KiB\n",
      "25/05/20 03:02:15 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/20 03:02:16 WARN DAGScheduler: Broadcasting large task binary with size 1249.7 KiB\n",
      "25/05/20 03:02:17 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB\n",
      "25/05/20 03:02:18 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/20 03:02:20 WARN DAGScheduler: Broadcasting large task binary with size 9.0 MiB\n",
      "25/05/20 03:02:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:24 WARN DAGScheduler: Broadcasting large task binary with size 10.4 MiB\n",
      "25/05/20 03:02:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:28 WARN DAGScheduler: Broadcasting large task binary with size 11.5 MiB\n",
      "25/05/20 03:02:29 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:31 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "25/05/20 03:02:33 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:35 WARN DAGScheduler: Broadcasting large task binary with size 11.5 MiB\n",
      "25/05/20 03:02:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:39 WARN DAGScheduler: Broadcasting large task binary with size 15.2 MiB\n",
      "25/05/20 03:02:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:44 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "25/05/20 03:02:46 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/20 03:02:48 WARN DAGScheduler: Broadcasting large task binary with size 10.4 MiB\n",
      "25/05/20 03:02:49 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:51 WARN DAGScheduler: Broadcasting large task binary with size 15.4 MiB\n",
      "25/05/20 03:02:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/20 03:02:55 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "25/05/20 03:02:56 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/05/20 03:02:58 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "25/05/20 03:02:59 WARN DAGScheduler: Broadcasting large task binary with size 1672.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_rf = pipeline_rf.fit(train_data)\n",
    "predictions_rf = model_rf.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "571e1ee5-39e7-4c51-816e-bb9a91b107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbb4db29-9a4f-46e8-b978-18d440fafc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 03:03:05 WARN DAGScheduler: Broadcasting large task binary with size 42.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1795888035670052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 03:10:03 WARN DAGScheduler: Broadcasting large task binary with size 42.6 MiB\n",
      "[Stage 110:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.07000723357622728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest Results:\")\n",
    "print(f\"Accuracy: {evaluator_accuracy.evaluate(predictions_rf)}\")\n",
    "print(f\"F1-Score: {evaluator_f1.evaluate(predictions_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbbf65ff-9d0a-4ee3-9a4b-172a1af9cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = pipeline_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a35fd552-2e61-4fe8-a54b-08e17e308743",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03d5916d-a5cc-4e68-a6ce-3d72c099aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16856576665840972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.055680547592227986\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {evaluator_accuracy.evaluate(lr_predictions)}\")\n",
    "print(f\"F1-Score: {evaluator_f1.evaluate(lr_predictions)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1bcab573-a0a6-40cd-9d08-30906a137bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263793a-7eeb-41a7-884c-a3c8eb759779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_converter = IndexToString(\n",
    "#     inputCol=\"prediction\",\n",
    "#     outputCol=\"predicted_h3_09\",\n",
    "#     labels=label_indexer.labels\n",
    "# )\n",
    "\n",
    "# final_predictions = label_converter.transform(gbt_predictions)\n",
    "# final_predictions.select(\"h3_09\", \"predicted_h3_09\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d29e549-f15e-4c36-b7dd-67ff07522d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.7 (main, Mar 20 2025, 00:23:21) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bcf360-a3cf-4000-a2be-50bc4cf12420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/team3/.local/lib/python3.6/site-packages (1.10.1)\n",
      "Requirement already satisfied: torchvision in /home/team3/.local/lib/python3.6/site-packages (0.11.2)\n",
      "Requirement already satisfied: pytorch_lightning in /home/team3/.local/lib/python3.6/site-packages (1.5.10.post0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/team3/.local/lib/python3.6/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: setuptools==59.5.0 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib64/python3.6/site-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.6/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/site-packages (from pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (2.10.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/site-packages (from pytorch_lightning) (4.64.1)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (2022.1.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/team3/.local/lib/python3.6/site-packages (from pytorch_lightning) (0.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /home/team3/.local/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging>=17.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.7)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.48.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/team3/.local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.3)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/site-packages (from tqdm>=4.41.0->pytorch_lightning) (5.4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/team3/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/team3/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/team3/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.9.1)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (1.26.18)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/team3/.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/team3/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/team3/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.41.0->pytorch_lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/team3/.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user torch torchvision pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c180b62-49ae-41cd-88be-1c3375f31089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu102\n",
      "0.11.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)         # Должно быть 1.13.1\n",
    "print(torchvision.__version__)   # Должно быть 0.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "470241ab-6ecd-4373-a4f6-bc1bf088160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5f13ef-1e73-4ecc-a92a-7755d32872cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # Должен показывать путь к Python 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f56947-3bd6-483d-b8ca-a6414444e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import models\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309cd77e-17eb-482c-881f-219a82bcfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = np.array(data.select(\"features\").collect()).squeeze()\n",
    "        self.labels = np.array(data.select(\"label\").collect()).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]]).squeeze()\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d214036-0980-4312-abcc-5e11c6958993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.01)\n",
    "        self.fc2 = nn.Linear(2100, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a285ed3-f9d9-4986-bd76-1a778c341c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline(stages=[mcc_indexer, assembler])\n",
    "preprocessed_data = preprocessing_pipeline.fit(data).transform(data)\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = np.array(data.select(\"features\").collect()).squeeze()\n",
    "        self.labels = np.array(data.select(\"label\").collect()).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]]).squeeze()\n",
    "        return features, label\n",
    "\n",
    "train_dataset = SparkDataset(train_data)\n",
    "test_dataset = SparkDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cce770a5-2047-4143-a4b0-845a30af8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(feature_cols)\n",
    "num_classes = data.select(\"label\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a171cf3-14ff-4bf0-ac12-354647e3a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNN(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8b1a53d-c9c1-4334-beef-415bc3a43a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b7fe0d6-4eb2-4bad-8f59-727625a0acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d878c1-077d-473b-be9b-8966643707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22953be6-34f6-44c8-a4ba-f34c2bd1d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Accuracy: 16.45%\n",
      "Epoch [2/10], Accuracy: 16.78%\n",
      "Epoch [3/10], Accuracy: 16.80%\n",
      "Epoch [4/10], Accuracy: 16.77%\n",
      "Epoch [5/10], Accuracy: 16.82%\n",
      "Epoch [6/10], Accuracy: 16.81%\n",
      "Epoch [7/10], Accuracy: 16.87%\n",
      "Epoch [8/10], Accuracy: 16.81%\n",
      "Epoch [9/10], Accuracy: 16.67%\n",
      "Epoch [10/10], Accuracy: 16.85%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f2fbde7-59c6-4a56-8ed8-12764d2e3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col, struct \n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def predict_batch(batch_iter):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for features, _ in batch_iter:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.numpy())\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "predict_udf = pandas_udf(predict_batch, returnType=DoubleType())\n",
    "\n",
    "final_predictions = test_data.withColumn(\n",
    "    \"nn_prediction\",\n",
    "    predict_udf(struct(*feature_cols))  \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "829c3659-32cc-4a08-8641-5f756b485e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "final_predictions = final_predictions.withColumnRenamed(\"nn_prediction\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acab72c6-2071-49e9-9771-8581f121a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4b0e941-5ac3-486e-9e5d-6883fe3f0d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyarrow\n",
    "print(pyarrow.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fc867-f3e7-4f49-941d-af9cf26d28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator_accuracy.evaluate(final_predictions)\n",
    "f1 = evaluator_f1.evaluate(final_predictions)\n",
    "\n",
    "print(\"Neural Network Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a59754f5-dbb4-483c-916f-3e0d2c6983a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8007639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "851f0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TabularResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.block1 = self._make_residual_block(input_size, 512)\n",
    "        self.block2 = self._make_residual_block(512, 256)\n",
    "        self.block3 = self._make_residual_block(256, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def _make_residual_block(self, in_features, out_features):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.block3(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8a2c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_size = len(feature_cols) \n",
    "num_classes = data.select(\"label\").distinct().count()\n",
    "model = TabularResNet(input_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1030d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# def prepare_data(df):\n",
    "#     features = np.stack(df.select(\"features\").toPandas()['features'].apply(lambda x: x.toArray()))\n",
    "#     labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "#     return SparkDataset(features, labels)\n",
    "\n",
    "# train_dataset = prepare_data(train_data)\n",
    "# test_dataset = prepare_data(test_data)\n",
    "\n",
    "preprocessing_pipeline = Pipeline(stages=[mcc_indexer, assembler])\n",
    "preprocessed_data = preprocessing_pipeline.fit(data).transform(data)\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = np.array(data.select(\"features\").collect()).squeeze()\n",
    "        self.labels = np.array(data.select(\"label\").collect()).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]]).squeeze()\n",
    "        return features, label\n",
    "\n",
    "train_dataset = SparkDataset(train_data)\n",
    "test_dataset = SparkDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1ab1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6388294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4.8850\n",
      "Epoch 2/20, Loss: 4.8808\n",
      "Epoch 3/20, Loss: 4.8779\n",
      "Epoch 4/20, Loss: 4.8727\n",
      "Epoch 5/20, Loss: 4.8666\n",
      "Epoch 6/20, Loss: 4.8685\n",
      "Epoch 7/20, Loss: 4.8588\n",
      "Epoch 8/20, Loss: 4.8605\n",
      "Epoch 9/20, Loss: 4.8528\n",
      "Epoch 10/20, Loss: 4.8537\n",
      "Epoch 11/20, Loss: 4.8497\n",
      "Epoch 12/20, Loss: 4.8400\n",
      "Epoch 13/20, Loss: 4.8422\n",
      "Epoch 14/20, Loss: 4.8351\n",
      "Epoch 15/20, Loss: 4.8367\n",
      "Epoch 16/20, Loss: 4.8262\n",
      "Epoch 17/20, Loss: 4.8266\n",
      "Epoch 18/20, Loss: 4.8223\n",
      "Epoch 19/20, Loss: 4.8158\n",
      "Epoch 20/20, Loss: 4.8140\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7762abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20ed7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c58a803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Defaulting to user installation because normal site-packages is not writeable',\n",
       " 'Collecting numpy',\n",
       " '  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)',\n",
       " 'Installing collected packages: numpy',\n",
       " 'Successfully installed numpy-1.19.5']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0594e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1656, F1-Score: 0.0002\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            preds = output.argmax(dim=1)\n",
    "            \n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(target.cpu().tolist())\n",
    "    \n",
    "    \n",
    "    correct = sum(1 for p, l in zip(all_preds, all_labels) if p == l)\n",
    "    accuracy = correct / len(all_labels)\n",
    "    \n",
    "    \n",
    "    unique_labels = set(all_labels)\n",
    "    f1_scores = []\n",
    "    for label in unique_labels:\n",
    "        TP = sum((p == label) and (l == label) for p, l in zip(all_preds, all_labels))\n",
    "        FP = sum((p == label) and (l != label) for p, l in zip(all_preds, all_labels))\n",
    "        FN = sum((p != label) and (l == label) for p, l in zip(all_preds, all_labels))\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    \n",
    "    return accuracy, avg_f1\n",
    "\n",
    "\n",
    "accuracy, f1 = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc2f178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
